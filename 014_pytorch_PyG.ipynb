{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385174b3-8660-41b5-b67b-8a061c2e95cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kostanew/anaconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ”¹ ESOL â€” Solubility Prediction with GNN (PyTorch Geometric)\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383c26d0-d22f-4566-a294-3065440a665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Load Dataset\n",
    "# ============================================\n",
    "\n",
    "df = pd.read_csv(\"delaney-processed.csv\")\n",
    "\n",
    "# Normalize target (recommended for regression stability)\n",
    "y_mean = df[\"measured log solubility in mols per litre\"].mean()\n",
    "y_std = df[\"measured log solubility in mols per litre\"].std()\n",
    "df[\"y_norm\"] = (df[\"measured log solubility in mols per litre\"] - y_mean) / y_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1aa6899-51b4-4ec0-8bfa-87a3d0ef05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. Molecule â†’ Graph conversion\n",
    "# ============================================\n",
    "\n",
    "def atom_features(atom):\n",
    "    \"\"\"Return a basic set of atom-level features.\"\"\"\n",
    "    return [\n",
    "        atom.GetAtomicNum(),\n",
    "        atom.GetTotalDegree(),\n",
    "        atom.GetFormalCharge(),\n",
    "        atom.GetTotalNumHs(),\n",
    "        atom.GetIsAromatic() * 1.0,\n",
    "    ]\n",
    "\n",
    "\n",
    "def bond_features(bond):\n",
    "    \"\"\"Return bond type as numerical encoding.\"\"\"\n",
    "    bt = bond.GetBondType()\n",
    "    return [\n",
    "        bt == Chem.rdchem.BondType.SINGLE,\n",
    "        bt == Chem.rdchem.BondType.DOUBLE,\n",
    "        bt == Chem.rdchem.BondType.TRIPLE,\n",
    "        bt == Chem.rdchem.BondType.AROMATIC,\n",
    "    ]\n",
    "\n",
    "\n",
    "def mol_to_graph(smiles, y):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    # --- Atom features ---\n",
    "    x = torch.tensor([atom_features(a) for a in mol.GetAtoms()], dtype=torch.float32)\n",
    "\n",
    "    # --- Bond connections ---\n",
    "    src, dst, e_feat = [], [], []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        src += [i, j]\n",
    "        dst += [j, i]\n",
    "        b = bond_features(bond)\n",
    "        e_feat += [b, b]\n",
    "\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(e_feat, dtype=torch.float32)\n",
    "\n",
    "    y = torch.tensor([y], dtype=torch.float32)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e36c694-a0b2-4520-a6c0-8f7caf16771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# 3. Custom PyG dataset\n",
    "# ===========================================================\n",
    "\n",
    "class EsolDataset(InMemoryDataset):\n",
    "    def __init__(self, dataframe, transform=None, pre_transform=None):\n",
    "        super().__init__(\".\", transform, pre_transform)\n",
    "        data_list = []\n",
    "        for smiles, target in zip(dataframe[\"smiles\"], dataframe[\"y_norm\"]):\n",
    "            g = mol_to_graph(smiles, target)\n",
    "            if g is not None:\n",
    "                data_list.append(g)\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3344545-9545-45d1-8cea-1002b255da4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_144874/1775182624.py:16: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  return self.data.y.size(0)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4. Train/test split + loaders (âœ… compatible fix)\n",
    "# ============================================\n",
    "\n",
    "dataset = EsolDataset(df)\n",
    "train_idx, test_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "# --- manually rebuild subsets (safe across PyG versions)\n",
    "train_dataset = [dataset.get(i) for i in train_idx]\n",
    "test_dataset = [dataset.get(i) for i in test_idx]\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "train_dataset = dataset.index_select(torch.tensor(train_idx))\n",
    "test_dataset = dataset.index_select(torch.tensor(test_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a1f5da-2f7c-4bb7-843c-958edc8171af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# 5. Model definition (GCNConv-based MPNN)\n",
    "# ===========================================================\n",
    "\n",
    "class MPNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = nn.Linear(hidden_channels, hidden_channels // 2)\n",
    "        self.lin2 = nn.Linear(hidden_channels // 2, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        return self.lin2(x).squeeze(-1)  # âœ… fix shape mismatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1af06e7-cccc-4678-ac55-749f6f70e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# 6. Setup training components\n",
    "# ===========================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MPNN(in_channels=5, hidden_channels=128).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=20, factor=0.7)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f1ed2a-da88-4ced-a445-bfe68f6cab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# 7. Train / Evaluate functions\n",
    "# ===========================================================\n",
    "\n",
    "def train_one_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_fn(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data)\n",
    "        preds.append(pred.cpu())\n",
    "        targets.append(data.y.cpu())\n",
    "    preds = torch.cat(preds).squeeze()\n",
    "    targets = torch.cat(targets).squeeze()\n",
    "    mse = mean_squared_error(targets, preds)\n",
    "    rmse = mse ** 0.5\n",
    "    r2 = r2_score(targets, preds)\n",
    "    return rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be548a06-c36f-48bf-ab49-9fbba7c91088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 | TrainLoss: 0.7373 | RMSE: 0.9097 | RÂ²: 0.2305\n",
      "Epoch 020 | TrainLoss: 0.5616 | RMSE: 0.7695 | RÂ²: 0.4495\n",
      "Epoch 030 | TrainLoss: 0.4421 | RMSE: 0.7514 | RÂ²: 0.4750\n",
      "Epoch 040 | TrainLoss: 0.4204 | RMSE: 0.6241 | RÂ²: 0.6379\n",
      "Epoch 050 | TrainLoss: 0.3840 | RMSE: 0.6033 | RÂ²: 0.6616\n",
      "Epoch 060 | TrainLoss: 0.3352 | RMSE: 0.5984 | RÂ²: 0.6671\n",
      "Epoch 070 | TrainLoss: 0.3645 | RMSE: 0.6115 | RÂ²: 0.6523\n",
      "Epoch 080 | TrainLoss: 0.3158 | RMSE: 0.5712 | RÂ²: 0.6966\n",
      "Epoch 090 | TrainLoss: 0.3060 | RMSE: 0.6352 | RÂ²: 0.6248\n",
      "Epoch 100 | TrainLoss: 0.4030 | RMSE: 0.5899 | RÂ²: 0.6764\n",
      "Epoch 110 | TrainLoss: 0.2929 | RMSE: 0.5664 | RÂ²: 0.7017\n",
      "Epoch 120 | TrainLoss: 0.2908 | RMSE: 0.5607 | RÂ²: 0.7076\n",
      "Epoch 130 | TrainLoss: 0.2855 | RMSE: 0.5505 | RÂ²: 0.7182\n",
      "Epoch 140 | TrainLoss: 0.2755 | RMSE: 0.5424 | RÂ²: 0.7265\n",
      "Epoch 150 | TrainLoss: 0.3028 | RMSE: 0.5392 | RÂ²: 0.7296\n",
      "Epoch 160 | TrainLoss: 0.2832 | RMSE: 0.5394 | RÂ²: 0.7294\n",
      "Epoch 170 | TrainLoss: 0.2608 | RMSE: 0.5462 | RÂ²: 0.7226\n",
      "Epoch 180 | TrainLoss: 0.2570 | RMSE: 0.5291 | RÂ²: 0.7397\n",
      "Epoch 190 | TrainLoss: 0.2550 | RMSE: 0.5238 | RÂ²: 0.7448\n",
      "Epoch 200 | TrainLoss: 0.2761 | RMSE: 0.5478 | RÂ²: 0.7210\n",
      "Epoch 210 | TrainLoss: 0.2431 | RMSE: 0.5200 | RÂ²: 0.7486\n",
      "Epoch 220 | TrainLoss: 0.2302 | RMSE: 0.5114 | RÂ²: 0.7569\n",
      "Epoch 230 | TrainLoss: 0.2228 | RMSE: 0.5576 | RÂ²: 0.7109\n",
      "Epoch 240 | TrainLoss: 0.2225 | RMSE: 0.5346 | RÂ²: 0.7343\n",
      "Epoch 250 | TrainLoss: 0.2266 | RMSE: 0.5246 | RÂ²: 0.7441\n",
      "Epoch 260 | TrainLoss: 0.1895 | RMSE: 0.4983 | RÂ²: 0.7692\n",
      "Epoch 270 | TrainLoss: 0.2035 | RMSE: 0.4941 | RÂ²: 0.7730\n",
      "Epoch 280 | TrainLoss: 0.1816 | RMSE: 0.4931 | RÂ²: 0.7739\n",
      "Epoch 290 | TrainLoss: 0.1690 | RMSE: 0.4902 | RÂ²: 0.7765\n",
      "Epoch 300 | TrainLoss: 0.1721 | RMSE: 0.4936 | RÂ²: 0.7735\n",
      "Epoch 310 | TrainLoss: 0.1731 | RMSE: 0.4764 | RÂ²: 0.7890\n",
      "Epoch 320 | TrainLoss: 0.1558 | RMSE: 0.4969 | RÂ²: 0.7704\n",
      "Epoch 330 | TrainLoss: 0.1766 | RMSE: 0.4894 | RÂ²: 0.7773\n",
      "Epoch 340 | TrainLoss: 0.1511 | RMSE: 0.4850 | RÂ²: 0.7813\n",
      "Epoch 350 | TrainLoss: 0.1556 | RMSE: 0.4892 | RÂ²: 0.7775\n",
      "Epoch 360 | TrainLoss: 0.1463 | RMSE: 0.4811 | RÂ²: 0.7848\n",
      "Epoch 370 | TrainLoss: 0.1415 | RMSE: 0.4909 | RÂ²: 0.7759\n",
      "Epoch 380 | TrainLoss: 0.1413 | RMSE: 0.5098 | RÂ²: 0.7583\n",
      "Epoch 390 | TrainLoss: 0.1416 | RMSE: 0.4708 | RÂ²: 0.7939\n",
      "Epoch 400 | TrainLoss: 0.1363 | RMSE: 0.4698 | RÂ²: 0.7947\n",
      "Epoch 410 | TrainLoss: 0.1313 | RMSE: 0.4694 | RÂ²: 0.7951\n",
      "Epoch 420 | TrainLoss: 0.1291 | RMSE: 0.4660 | RÂ²: 0.7981\n",
      "Epoch 430 | TrainLoss: 0.1295 | RMSE: 0.4697 | RÂ²: 0.7949\n",
      "Epoch 440 | TrainLoss: 0.1299 | RMSE: 0.4674 | RÂ²: 0.7969\n",
      "Epoch 450 | TrainLoss: 0.1271 | RMSE: 0.4699 | RÂ²: 0.7947\n",
      "Epoch 460 | TrainLoss: 0.1324 | RMSE: 0.4717 | RÂ²: 0.7931\n",
      "Epoch 470 | TrainLoss: 0.1239 | RMSE: 0.4668 | RÂ²: 0.7974\n",
      "Epoch 480 | TrainLoss: 0.1267 | RMSE: 0.4748 | RÂ²: 0.7904\n",
      "Epoch 490 | TrainLoss: 0.1329 | RMSE: 0.4740 | RÂ²: 0.7911\n",
      "Epoch 500 | TrainLoss: 0.1232 | RMSE: 0.4634 | RÂ²: 0.8003\n",
      "Epoch 510 | TrainLoss: 0.1250 | RMSE: 0.4696 | RÂ²: 0.7950\n",
      "Epoch 520 | TrainLoss: 0.1200 | RMSE: 0.4633 | RÂ²: 0.8004\n",
      "Epoch 530 | TrainLoss: 0.1195 | RMSE: 0.4661 | RÂ²: 0.7980\n",
      "Epoch 540 | TrainLoss: 0.1191 | RMSE: 0.4635 | RÂ²: 0.8002\n",
      "Epoch 550 | TrainLoss: 0.1171 | RMSE: 0.4634 | RÂ²: 0.8003\n",
      "Epoch 560 | TrainLoss: 0.1163 | RMSE: 0.4602 | RÂ²: 0.8031\n",
      "Epoch 570 | TrainLoss: 0.1194 | RMSE: 0.4628 | RÂ²: 0.8008\n",
      "Epoch 580 | TrainLoss: 0.1174 | RMSE: 0.4587 | RÂ²: 0.8043\n",
      "Epoch 590 | TrainLoss: 0.1155 | RMSE: 0.4655 | RÂ²: 0.7985\n",
      "Epoch 600 | TrainLoss: 0.1183 | RMSE: 0.4627 | RÂ²: 0.8009\n",
      "Epoch 610 | TrainLoss: 0.1150 | RMSE: 0.4609 | RÂ²: 0.8025\n",
      "Epoch 620 | TrainLoss: 0.1137 | RMSE: 0.4587 | RÂ²: 0.8044\n",
      "Epoch 630 | TrainLoss: 0.1128 | RMSE: 0.4594 | RÂ²: 0.8038\n",
      "Epoch 640 | TrainLoss: 0.1139 | RMSE: 0.4604 | RÂ²: 0.8029\n",
      "Epoch 650 | TrainLoss: 0.1126 | RMSE: 0.4594 | RÂ²: 0.8038\n",
      "Epoch 660 | TrainLoss: 0.1119 | RMSE: 0.4592 | RÂ²: 0.8039\n",
      "Epoch 670 | TrainLoss: 0.1118 | RMSE: 0.4588 | RÂ²: 0.8042\n",
      "Epoch 680 | TrainLoss: 0.1108 | RMSE: 0.4577 | RÂ²: 0.8052\n",
      "Epoch 690 | TrainLoss: 0.1109 | RMSE: 0.4576 | RÂ²: 0.8053\n",
      "Epoch 700 | TrainLoss: 0.1116 | RMSE: 0.4579 | RÂ²: 0.8050\n",
      "Epoch 710 | TrainLoss: 0.1108 | RMSE: 0.4583 | RÂ²: 0.8047\n",
      "Epoch 720 | TrainLoss: 0.1113 | RMSE: 0.4571 | RÂ²: 0.8058\n",
      "Epoch 730 | TrainLoss: 0.1101 | RMSE: 0.4577 | RÂ²: 0.8052\n",
      "Epoch 740 | TrainLoss: 0.1103 | RMSE: 0.4571 | RÂ²: 0.8057\n",
      "Epoch 750 | TrainLoss: 0.1101 | RMSE: 0.4569 | RÂ²: 0.8059\n",
      "Epoch 760 | TrainLoss: 0.1106 | RMSE: 0.4573 | RÂ²: 0.8056\n",
      "Epoch 770 | TrainLoss: 0.1096 | RMSE: 0.4571 | RÂ²: 0.8057\n",
      "Epoch 780 | TrainLoss: 0.1097 | RMSE: 0.4571 | RÂ²: 0.8057\n",
      "Epoch 790 | TrainLoss: 0.1095 | RMSE: 0.4569 | RÂ²: 0.8059\n",
      "Epoch 800 | TrainLoss: 0.1097 | RMSE: 0.4567 | RÂ²: 0.8061\n",
      "Epoch 810 | TrainLoss: 0.1093 | RMSE: 0.4569 | RÂ²: 0.8059\n",
      "Epoch 820 | TrainLoss: 0.1092 | RMSE: 0.4571 | RÂ²: 0.8057\n",
      "Epoch 830 | TrainLoss: 0.1093 | RMSE: 0.4569 | RÂ²: 0.8059\n",
      "Epoch 840 | TrainLoss: 0.1093 | RMSE: 0.4569 | RÂ²: 0.8059\n",
      "Epoch 850 | TrainLoss: 0.1092 | RMSE: 0.4568 | RÂ²: 0.8060\n",
      "Epoch 860 | TrainLoss: 0.1092 | RMSE: 0.4569 | RÂ²: 0.8059\n",
      "Epoch 870 | TrainLoss: 0.1091 | RMSE: 0.4569 | RÂ²: 0.8059\n",
      "Epoch 880 | TrainLoss: 0.1091 | RMSE: 0.4569 | RÂ²: 0.8059\n",
      "Epoch 890 | TrainLoss: 0.1092 | RMSE: 0.4568 | RÂ²: 0.8060\n",
      "Epoch 900 | TrainLoss: 0.1091 | RMSE: 0.4568 | RÂ²: 0.8060\n",
      "Epoch 910 | TrainLoss: 0.1090 | RMSE: 0.4568 | RÂ²: 0.8060\n",
      "Epoch 920 | TrainLoss: 0.1090 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 930 | TrainLoss: 0.1090 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 940 | TrainLoss: 0.1090 | RMSE: 0.4568 | RÂ²: 0.8060\n",
      "Epoch 950 | TrainLoss: 0.1090 | RMSE: 0.4568 | RÂ²: 0.8060\n",
      "Epoch 960 | TrainLoss: 0.1090 | RMSE: 0.4568 | RÂ²: 0.8060\n",
      "Epoch 970 | TrainLoss: 0.1090 | RMSE: 0.4568 | RÂ²: 0.8060\n",
      "Epoch 980 | TrainLoss: 0.1090 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 990 | TrainLoss: 0.1090 | RMSE: 0.4568 | RÂ²: 0.8060\n",
      "Epoch 1000 | TrainLoss: 0.1090 | RMSE: 0.4568 | RÂ²: 0.8060\n",
      "Epoch 1010 | TrainLoss: 0.1090 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1020 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1030 | TrainLoss: 0.1090 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1040 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1050 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1060 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1070 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1080 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1090 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1100 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1110 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1120 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1130 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1140 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1150 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1160 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1170 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1180 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1190 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1200 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1210 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1220 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1230 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1240 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1250 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1260 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1270 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1280 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1290 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1300 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1310 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1320 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1330 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1340 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1350 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1360 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1370 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1380 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1390 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1400 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1410 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1420 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1430 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1440 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1450 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1460 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1470 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1480 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1490 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1500 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1510 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1520 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1530 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1540 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1550 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1560 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1570 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1580 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1590 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1600 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1610 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1620 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1630 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1640 | TrainLoss: 0.1089 | RMSE: 0.4568 | RÂ²: 0.8060\n",
      "Epoch 1650 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1660 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1670 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1680 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1690 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1700 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1710 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1720 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1730 | TrainLoss: 0.1089 | RMSE: 0.4568 | RÂ²: 0.8060\n",
      "Epoch 1740 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1750 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1760 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1770 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1780 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1790 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1800 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n",
      "Epoch 1810 | TrainLoss: 0.1089 | RMSE: 0.4567 | RÂ²: 0.8060\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m best_rmse = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m5001\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     rmse, r2 = evaluate(test_loader)\n\u001b[32m      9\u001b[39m     scheduler.step(rmse)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(loader)\u001b[39m\n\u001b[32m     12\u001b[39m     loss = loss_fn(out, data.y)\n\u001b[32m     13\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     total_loss += loss.item() * data.num_graphs\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / \u001b[38;5;28mlen\u001b[39m(loader.dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.11/site-packages/torch/optim/optimizer.py:484\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    480\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    481\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    482\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    487\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.11/site-packages/torch/optim/optimizer.py:89\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     88\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     91\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.11/site-packages/torch/optim/adam.py:245\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    214\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    216\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    217\u001b[39m         group,\n\u001b[32m    218\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    223\u001b[39m         state_steps,\n\u001b[32m    224\u001b[39m     )\n\u001b[32m    226\u001b[39m     adam(\n\u001b[32m    227\u001b[39m         params_with_grad,\n\u001b[32m    228\u001b[39m         grads,\n\u001b[32m    229\u001b[39m         exp_avgs,\n\u001b[32m    230\u001b[39m         exp_avg_sqs,\n\u001b[32m    231\u001b[39m         max_exp_avg_sqs,\n\u001b[32m    232\u001b[39m         state_steps,\n\u001b[32m    233\u001b[39m         amsgrad=group[\u001b[33m\"\u001b[39m\u001b[33mamsgrad\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    234\u001b[39m         has_complex=has_complex,\n\u001b[32m    235\u001b[39m         beta1=beta1,\n\u001b[32m    236\u001b[39m         beta2=beta2,\n\u001b[32m    237\u001b[39m         lr=group[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    238\u001b[39m         weight_decay=group[\u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    239\u001b[39m         eps=group[\u001b[33m\"\u001b[39m\u001b[33meps\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    240\u001b[39m         maximize=group[\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    241\u001b[39m         foreach=group[\u001b[33m\"\u001b[39m\u001b[33mforeach\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    242\u001b[39m         capturable=group[\u001b[33m\"\u001b[39m\u001b[33mcapturable\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    243\u001b[39m         differentiable=group[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    244\u001b[39m         fused=group[\u001b[33m\"\u001b[39m\u001b[33mfused\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m         grad_scale=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgrad_scale\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    246\u001b[39m         found_inf=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfound_inf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    247\u001b[39m     )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# 8. Training loop\n",
    "# ===========================================================\n",
    "\n",
    "best_rmse = float(\"inf\")\n",
    "for epoch in range(1, 5001):\n",
    "    train_loss = train_one_epoch(train_loader)\n",
    "    rmse, r2 = evaluate(test_loader)\n",
    "    scheduler.step(rmse)\n",
    "\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        torch.save(model.state_dict(), \"best_esol_model.pt\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | TrainLoss: {train_loss:.4f} | RMSE: {rmse:.4f} | RÂ²: {r2:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Training complete! Best normalized RMSE:\", best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f31316c2-f937-4506-90b2-942bc39b95f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final (Denormalized) Performance: RMSE=0.957 | RÂ²=0.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_144874/312461329.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_esol_model.pt\"))\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# 9. Evaluate best model (denormalized)\n",
    "# ===========================================================\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_esol_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "preds, targets = [], []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data)\n",
    "        preds.append(pred.cpu())\n",
    "        targets.append(data.y.cpu())\n",
    "\n",
    "preds = torch.cat(preds).squeeze().numpy() * y_std + y_mean\n",
    "targets = torch.cat(targets).squeeze().numpy() * y_std + y_mean\n",
    "\n",
    "rmse_final = mean_squared_error(targets, preds) ** 0.5\n",
    "r2_final = r2_score(targets, preds)\n",
    "\n",
    "print(f\"\\nðŸ“Š Final (Denormalized) Performance: RMSE={rmse_final:.3f} | RÂ²={r2_final:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4090a-6533-4b06-8074-60271bed90cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (llm)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
